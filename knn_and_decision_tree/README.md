Name: Ryan Lindsey

Description:
  This package contains my implementation of the decision_tree and knn algorithms, with
  some code used for munging data and generating graphs. I've written a bunch of test cases
  using the unittest package in python3, and a Makefile for exercising my code. To run the
  code that displays the requirements for the assignment, run 'make run'. To run the test
  cases type 'make test'.

  I tried packaging this whole thing as an iPython notebook so I could properly explain my
  results, side-by-side with the code and graphs generated. But, I can't import my code from
  a notebook in this directory, and don't have enough time to figure it out. So, I'm going to
  explain the results of my program here.

Results:
  For my decision tree I had some difficulty getting good splits using the gini impurity 
  metric and splitting over an interval of points. My model gets fairly high accuracy if
  you use the entropy equation for making splits, and split on each unique feature instead
  of intervals in the feature space. However, the constructTree function in the decision
  tree package is parameterized to make it easy to change those parameters (assuming you
  want to see output for a different configuration).

  The test_missclassification.py file contains tests that will run my model and sklearns
  models against the same dataset, and then compare the misclassification error the two
  models produce. The tests will pass if the error rates are within 7 decimal places of
  each other. This test NEVER passes, but I've kept it to see a comparison of my error
  rates vs sklearns. 
  
  If you're unable to run my program, I've included graphs and a sample output for you
  to look over. For the most part, it seems like my decision boundaries are close to the
  ones generated by sklearn. However, for synthetic-4, my KNN algorithm has a hard time
  drawing the decision boundary for that dataset. If you look at the misclassification 
  rates between my KNN and sklearn's implementation, it seems like mine is classifying 
  a lot of points correctly. This makes me think that there's a problem with the
  graphing of my KNN.
  
Files:
    main.py - Main driver. Prints out basic requirements for assignment, generates graphs
      that are displayed to the user, and also saved in the graphs/ directory. 
    
    Makefile - Basic commands for running the program and launching the test apparatus.
    
    README.md - This file.
    
    requirements.txt - requirements for the pip3 package manager.

    skl.py - Test file I used during development, mostly for testing my models against
      the models implemented in sklearn. Right now it contains some code for finding the
      optimal depth for the decision tree, but I didn't finish my analysis of that. 

  cross_validator:
    cv.py - contains a couple of equations used in cross validation. I meant to implement
      a bunch of CV methods in this file, but ended up coding them into main.py.

  data:
    Unzipped hw1_data.zip file.

  data_wrangler:
    data_wrangler.py - contains methods for pulling data from the files and splitting 
      files into labels/features properly. Also contains a method for shuffling data.

  decision_tree:
    equations.py - contains some equations for running the decision_tree, like entropy,
      gini, etc.
    
    tree.py - contains implementation of the decision tree.
  
  graphs:
    contains graphs generated by main.py.

  knn:
    knn.py - main implementation of kNN. Contains functions used by kNN.

  sketch_artist:
    graph.py - contains functions for generating graphs.

  test:
    contains a bunch of files used for testing. The test_missclassification file seems
    to always fail (my models never come within 7 decimal places of accuracy of the 
    sklearn models)

TODO:
    Change the structure of the code. Maybe consolidate everything to one directory. 
