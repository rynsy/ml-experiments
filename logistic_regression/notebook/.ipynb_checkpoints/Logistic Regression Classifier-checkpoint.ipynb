{
 "metadata": {
  "name": "",
  "signature": "sha256:72944d88fc63965430fc3067880c16b62464173f0ca5e1422daa0a648a230c55"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    \"\"\"\n",
      "        learn : learning rate for gradient descent\n",
      "        iterations: number of iterations for gradient descent\n",
      "    \"\"\"\n",
      "    def __init__(self, learn=0.01, iterations=50):\n",
      "        self.learn = learn\n",
      "        self.iterations = iterations\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        self.w = np.zeros(1 + X.shape[1])\n",
      "        self.cost = []\n",
      "        for i in range(self.iterations):\n",
      "            y_hat = self.activation(X)\n",
      "            error = (y - y_hat)\n",
      "            n_gradient = X.T.dot(error) #negative gradient\n",
      "            self.w[1:] += self.learn * n_gradient\n",
      "            self.w[0] += self.learn * sum(error)\n",
      "            self.cost.append(self.logit_cost(y, self.activation(X)))\n",
      "        return self\n",
      "    \n",
      "    def logit_cost(self, y, y_hat):\n",
      "        return - y.dot(np.log(y_hat)) - ((1 - y).dot(np.log(1 - y_hat)))\n",
      "\n",
      "    def sigmoid(self, x):\n",
      "        return 1. / (1. + np.exp(-x))\n",
      "\n",
      "    def net_input(self, X):\n",
      "        return np.dot(X, self.w[1:]) + self.w[0]\n",
      "\n",
      "    def activation(self, X):\n",
      "        return self.sigmoid(self.net_input(X))\n",
      "\n",
      "    def predict_proba(self, X):\n",
      "        \"\"\"\n",
      "            similar to sklearn implementation\n",
      "        \"\"\"\n",
      "        return self.activation(X)\n",
      "\n",
      "    def predict(self, X):\n",
      "        return np.where(self.net_input(X) >= 0.0, 1, 0)\n",
      "        #return np.where(self.activation(X) >= THRESH, 1, 0)\n",
      "\n",
      "    def misclassification(self, X, y):\n",
      "        \"\"\"\n",
      "            incorrect predictions / total examples\n",
      "        \"\"\"\n",
      "        res = self.predict(X)\n",
      "        return sum(map(lambda (x,y): x != y, zip(y,res))) / len(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generateFigures(expansion_func, graphName=\"part1\"):\n",
      "    graphs = plt.figure()\n",
      "    for i in range(1,7):\n",
      "        data = loadSynthetic(i)\n",
      "        X,Y = data.iloc[:,0:2], data.iloc[:,-1]\n",
      "        colorize = lambda d: map(lambda x: \"r\" if x == 1 else \"b\", d) #only need two labels/colors\n",
      "        h = 0.2 #mesh stepsize\n",
      "        \n",
      "        clf = LogisticRegression() #todo changeme\n",
      "        clf = clf.fit(expansion_func(X.iloc[:,0],X.iloc[:,1]), train_labels) #todo changeme\n",
      "        \n",
      "        x_min, x_max = X.iloc[:,0].min() - 1, X.iloc[:,0].max() + 1\n",
      "        y_min, y_max = X.iloc[:,1].min() - 1, X.iloc[:,1].max() + 1\n",
      "\n",
      "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "        Z = clf.predict_proba(expansion_func(xx,yy)) #todo changeme\n",
      "        Z = Z.reshape(xx.shape)\n",
      "    \n",
      "        fig = graphs.add_subplot(2,3,i)\n",
      "        fig.set_xlabel(\"X1\")\n",
      "        fig.set_ylabel(\"X2\")\n",
      "        fig.set_title(\"synthetic-\" + str(i))\n",
      "        plt.set_cmap('prism')\n",
      "        cs = plt.imshow(Z,\n",
      "                        extent=(x_min,x_max,y_max,y_min),\n",
      "                        cmap=plt.cm.jet)\n",
      "        plt.clim(0,1)\n",
      "        levels = np.array([.5])\n",
      "        cs_line = plt.contour(xx,yy,Z,levels)\n",
      "        plt.scatter(X.iloc[:,0], X.iloc[:,1], c=colorize(Y), edgecolors='k', cmap=plt.cm.jet)\n",
      "    CB = plt.colorbar(cs)\n",
      "    plt.savefig(\"figures/\" + str(graphName) + \".png\", dpi=300)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "expand = lambda x,y: np.stack((np.ones(x.size), # add the bias term    \n",
      "                                x.ravel(), # make the matrix into a vector\n",
      "                                y.ravel(),    \n",
      "                                np.array([m * n for (m,n) in zip(x.ravel(),y.ravel())]),\n",
      "                                x.ravel()**2,    \n",
      "                                y.ravel()**2),\n",
      "                                axis=1) # add a quadratic term for fun\n",
      "\n",
      "expandXY = lambda x,y: np.stack((np.ones(x.size), # add the bias term    \n",
      "                                np.array([m * n for (m,n) in zip(x.ravel(),y.ravel())]),\n",
      "                                x.ravel()**2,    \n",
      "                                y.ravel()**2),\n",
      "                                axis=1)\n",
      "\n",
      "unravel = lambda x,y: np.c_[x.ravel(), y.ravel()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}